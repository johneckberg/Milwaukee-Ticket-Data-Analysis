{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c74c022",
   "metadata": {},
   "source": [
    " Establish history of parking services in milwaukee?\n",
    "- Duncan but Duncan got partially dropped? Portal still used though\n",
    "- Route1 is the aplr contractor\n",
    "- Route1 uses Genetic AutoVU \n",
    "    - https://resources.genetec.com/en-product-brochures/genetec-autovu-license-plate-recognition\n",
    "        - Surprisingly low false positive rate?\n",
    "    - Does Genetec manage all of the data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fecb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d310f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kaleido #for PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93719bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef35e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24b86cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tickets = pd.read_csv(\"Tickets2012LatLngDropNa.csv\")\n",
    "tickets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451953f5",
   "metadata": {},
   "source": [
    "Below is v hacky and messy, but thats python! trying to get an understanding of the tickets given out across the city each day. Converting date to an int so I can plot the time series in one awful 3d mess. We can just use the date for most proccessing and other visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0219b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers = 10, progress_bar=False)\n",
    "\n",
    "def convert(date):\n",
    "    return datetime.strptime(date, \"%m/%d/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickettest_1 = tickets.head(25000)\n",
    "tickettest_1[\"DATETIME\"] = tickettest_1['ISSUEDATE'].parallel_apply(convert)\n",
    "\n",
    "fig = plt.figure(figsize =(30,30))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "zdata = tickettest_1[\"DATETIME\"].values.astype(\"float64\")\n",
    "xdata = tickettest_1['Lat']\n",
    "ydata = tickettest_1['Lng']\n",
    "ax.scatter3D( xdata, zdata, ydata,);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af034875",
   "metadata": {},
   "source": [
    "Now were cooking with gas! The first goal is to build a some better maps with street overlay and also cluster. probs gonna go with fuzzy c means, and I would ideally rewrite this as a kotlin app, so I am looking for libraries that also have high levels of jvm support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d6eb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandarallel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    tickets, geometry=gpd.points_from_xy(tickets.Lng, tickets.Lat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b1616e",
   "metadata": {},
   "source": [
    "Lets plot our ticket data as a time series to see how things change over the year! This can inform us on what patterns may exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56a40c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "token = \"pk.eyJ1Ijoiam9obmVja2JlcmciLCJhIjoiY2xlbHJoMWViMHkweDNvbm1tZzA2NHA1cSJ9.IXat9PfPeqR3jd4uW4TETQ\"\n",
    "#saftey shmaftey\n",
    "\n",
    "\n",
    "fig = px.scatter_mapbox( # yes i know lat and lon are flipped \n",
    "    tickets,\n",
    "    lat=\"Lng\",\n",
    "    lon=\"Lat\",\n",
    "    hover_data=[\"ISSUETIME\"],\n",
    "    animation_frame=\"ISSUEDATE\",\n",
    ").update_layout(mapbox_style=\"streets\",margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0}, mapbox_accesstoken=token)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae031b72",
   "metadata": {},
   "source": [
    "Lets quick export this as a gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c36b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate frames\n",
    "frames = []\n",
    "for s, fr in enumerate(fig.frames):\n",
    "    # set main traces to appropriate traces within plotly frame\n",
    "    fig.update(data=fr.data)\n",
    "    # move slider to correct place\n",
    "    fig.layout.sliders[0].update(active=s)\n",
    "    # generate image of current state\n",
    "    frames.append(PIL.Image.open(io.BytesIO(fig.to_image(format=\"png\"))))\n",
    "    \n",
    "# create animated GIF\n",
    "frames[0].save(\n",
    "        \"MilwaukeeParking2012Vis.gif\",\n",
    "        save_all=True,\n",
    "        append_images=frames[1:],\n",
    "        optimize=True,\n",
    "        duration=500,\n",
    "        loop=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebe4bcf",
   "metadata": {},
   "source": [
    "Next lets do a heatmap over the course of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3afebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "heat = px.density_mapbox(tickets, lat='Lng', lon='Lat' z= \"\", radius=10, zoom=0,\n",
    "                        mapbox_style=\"street\").update_layout(mapbox_accesstoken=token)\n",
    "heat.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aa1dad",
   "metadata": {},
   "source": [
    "I gotta group by address then sum for the Z axis in the heat map, ill do that in a second\n",
    "First, I want to quick discuss different ways to predict a ticket given a location.\n",
    "\n",
    "1. A suggestion i recived was to first calculate the total number of cars that can be parked on each city block/ in front of each building, then use that to determine the probability on any given day. My concern with this is it assumes that every car parked is parked illegally, and has an equal oppurtunity to get ticketed.\n",
    "2. I think a better approach may be to think less about cars and more about regions of the city/map. Say we split the city up into its respective blocks, then into the street segments that define the blocks. Then, understanding the geographical impliciation that if a parking enforcement officer scans one car on a a street segment, they must scan the rest of the cars on the street segment, we focus on the likelyhood of a officer passing through a given street segment.\n",
    "\n",
    "- Theres this paper im readin that discusses a novel indicator, IPS or illegal parking score. How can we tune this for milwaukee? \n",
    "- the probability of a segment containing a parking ticket = likleyhood of your illegally parked car getting a  = (num of days segment gets a ticket/days in time frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dede4328",
   "metadata": {},
   "source": [
    "Lets look at some streets. these were taken from Milwaukees open data portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6102d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "streets = gpd.read_file('street.zip')\n",
    "streets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007fac5",
   "metadata": {},
   "source": [
    "We can defintely piece together street segments from this! First we can convert the linestring to points and then "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2886beec",
   "metadata": {},
   "source": [
    "adverserial regression? rnn GAN? how can we seperate the data into routes? street data into street segment data -> cluster -> time series GAN? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3521870f",
   "metadata": {},
   "source": [
    "Or just turn time features into columns (ie month 0-11, day 0-6, ticket type 0-2). then it will all be discrete tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9515686",
   "metadata": {},
   "source": [
    "Previous approaches:\n",
    "- Previous approach in Madison/ deep learning paper NYC data\n",
    "    - Note: RF performed great & they have provided some great insights into useful features and best practices for cleaning data\n",
    "        - They have a tendency to not overfit \n",
    "    - https://www.tandfonline.com/doi/full/10.1080/19475683.2019.1679882?scroll=top&needAccess=true&role=tab\n",
    "    - Regression for num of tickets given a location \n",
    "        - This might not really be useful for trying to decide a parking spot\n",
    "    - Why is treating this at a simple supervised binary classification task silly\n",
    "        - This leads to unbalanced and outright false data (they state 70 30 split, milwaukee will be hypothetically worse due to lower num of parking cops)\n",
    "    - Overall good accuracy, “Given a search location and time for on-street parking, the mean testing error is less than 3 tickets for regression and the F1-score is 0.82 for parking legality prediction”\n",
    "        - Can we do better?\n",
    "        - Also can we perform other correlation testing to see how poverty & snow & alternate side plays into it?\n",
    "\n",
    "    - DNN paper treats it as a time series prediction task  Thessaloniki, Greece data\n",
    "        - https://cidl.csd.auth.gr/resources/conference_pdfs/Deep%20Learning%20for%20On-Street%20Parking%20Violation.pdf\n",
    "        - Uses RNN\n",
    "        - Predicts ticket percentage given time unit and street with pretty high accuracy, but the percentage is based on the capacity of the given street segment of the specific sector at a specific time slot t. Not something that is easy to calculate for milwaukee or a lot of cities\n",
    "        - MAE of 0.169\n",
    "\n",
    "How many cars on a street? Based on total number of cars parked OPEN DATA go back and stop and think about a car proven. \n",
    "- This assumes that every car entering the city is parked on the street. This is false\n",
    "- But we could estimate number of cars on street with street length and mean car length + mean distance between cars (oh god I could gather that data by hand)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eb4e6a",
   "metadata": {},
   "source": [
    "But what if we take a generative, rather than purely regressive approach to parking ticket detection. Applying generative modeling to a classification task\n",
    "IDEA: GOOD SS-LEARNING MIGHT NEED A BAD GAN: https://arxiv.org/abs/1705.09783"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9495a7c",
   "metadata": {},
   "source": [
    "- Its important to note, we have an awful class imbalance like 1:800 in the case of milwaukee and given our time granularity, instead, lets frame problem in the context of novelty detection/ one class classification with respect to the distribution of our features?\n",
    "- A review of novelty detection (Pimento 2014)\n",
    "        - https://romisatriawahono.net/lecture/rm/survey/machine%20learning/Pimentel%20-%20Novelty%20Detection%20-%202014.pdf\n",
    "- A more recent (2021) review of OCC methods:\n",
    "    - https://arxiv.org/pdf/2101.03064.pdf \n",
    "    - One class SVM: SVDD?\n",
    "        - Doesn’t do well with high dimensional data? \n",
    "        - I would need to do heavy data processing, same with distance based methods\n",
    "            - Distance based would be computationally expensive \n",
    "    - Adversarially learned one class classifier: (needs multi class but could work with imbalanced class?)\n",
    "        -  https://arxiv.org/pdf/1802.09088v2.pdf\n",
    "        - One network works as the novelty detector, while the other supports it by enhancing the inlier samples and distorting the outliers. The intuition is that the separability of the enhanced inliers and distorted outliers is much better than deciding on the original samples.\n",
    "    - SSL GAN\n",
    "\n",
    "        - Cherti et al. (2016) empirically demonstrated that a GAN that was trained with a regular loss function is able to generate out-of-class realistic images, e.g., a GAN trained on MNIST digits generated Latin letters\n",
    "        - BIG IDEA: novelty detection based on generating novel examples given a feature matching loss function. If the generator is “estimating” the target distribution, how exactly does it do that? The conclusion is that only a bad generator, where pg (x) dne pdata(x) and which generates samples outside of the high-density areas of the real data distribution, is able to improve semi-supervised learning (Salimans et al. (2016)) what about unsupervised?? we want a mixture generator that will generate novel data distributed in nearby surrounding or at low-density regions of the true data manifold. \n",
    "            - Instead of directly maximizing the output of the discriminator, the new objective requires the generator to generate data that matches the statistics of the real data, where we use the discriminator only to specify the statistics that we think are worth matching. (Isn’t this what ARFs already do?when picking samples based on coverage? That’s why they don’t perform as well?) Specifically, we train the generator to match the expected value of the features on an intermediate layer of the discriminator. \n",
    "                - This is a natural choice of statistics for the generator to match, since by training the discriminator we ask it to find those features that are most discriminative of real data versus data generated by the current model. Specifically, we train the generator to match the expected value of the features on an intermediate layer of the discriminator. This is a natural choice of statistics for the generator to match, since by training the discriminator we ask it to find those features that are most discriminative of real data versus data generated by the current model.\n",
    "                    - (How close is this to choosing the “most relevant pure leaves and sampling from marginals? the generator’s sampling strategy drives original and synthetic data closer together by proof. Prove that this is true for ARFS fancy sampling scheme? Given a \n",
    "        - Gan for novelty detection: \n",
    "            - If during training the generator generates a mixture of known data and novel data, the discriminator learns to discriminate novel data from known data and essentially becomes a novelty detector. its proved in that paper that in this case the discriminator become an optimal novelty detector (for a given false positive rate) \n",
    "            - in other words the mixture generator is a generator that generates a mixture of true data distribution pata(x) and some other data pother (x), where at least part of the pother (x) probability mass is concentrated in lower-density regions of pdata(x).\n",
    "            - https://arxiv.org/pdf/1802.10560.pdf\n",
    "    - ARFs \n",
    "        - can we prove the tree sampling scheme is a mixture generator? can we graph the densities of real and fake and see how they overlap?\n",
    "        - The “generator” is a simple sampling scheme that draws from the marginals in adaptively selected subregions; the “discriminator” is a RF classifier. \n",
    "        - https://arxiv.org/pdf/2205.09435.pdf\n",
    "    - Deep Gan method for density estimation: can we change the loss function? Have others?\n",
    "        - https://www.pnas.org/doi/epdf/10.1073/pnas.2101344118\n",
    "    - CGAN for tabular data: can you connect this the the adversarially learned one class classifier? \n",
    "        - https://arxiv.org/pdf/1907.00503.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479d572c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
